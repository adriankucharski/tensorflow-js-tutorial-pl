{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Niestandardowa pętla uczenia modelu w Keras i TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poprzedni przykład pokazał, jak zbudować model GAN. Jednakże, aby zapewnić najwyższą wydajność i wykorzystać dobrodziejstwo kompilowania modelu przy pomocy `XLA`, musimy zaimplementować niestandardową pętlę uczenia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733433414.318855 1037836 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733433414.321955 1037836 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "I0000 00:00:1733433417.505067 1037836 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21769 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733433426.149458 1037914 service.cc:148] XLA service 0x7f7e30002d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733433426.149497 1037914 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "I0000 00:00:1733433427.293340 1037914 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1733433444.899059 1037914 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - d_loss: 2.0396 - g_loss: 0.5082"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers, losses, datasets, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Sequence\n",
    "\n",
    "\n",
    "class FeatureMatchingLoss(losses.Loss):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss = 0.0\n",
    "        # Ostatnie wyjście to wynik dlatego iterujemy do przedostatniego\n",
    "        for i in range(len(y_pred) - 1):\n",
    "            t, p = y_true[i], y_pred[i]\n",
    "            loss += tf.reduce_mean(tf.abs(t - p))\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Tworzenie generatora\n",
    "def get_generator(noise_size: int = 64, classes: int = 10) -> models.Model:\n",
    "    inputs = layers.Input(shape=[noise_size], dtype=tf.float32, name=\"noise\")\n",
    "    aux_inputs = layers.Input(shape=[1], dtype=tf.int32, name=\"category\")\n",
    "\n",
    "    # Embedding dla kategorii\n",
    "    y = layers.Embedding(classes, noise_size)(aux_inputs)\n",
    "\n",
    "    x = layers.Add()([inputs, y])\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape([4, 4, -1])(x)  # -1 automatycznie dobiera wymiar\n",
    "\n",
    "    for filters in [512, 256, 128]:\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.GroupNormalization(groups=-1)(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        x = layers.UpSampling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2D(3, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "    return models.Model(inputs=[inputs, aux_inputs], outputs=x, name=\"generator\")\n",
    "\n",
    "\n",
    "# Tworzenie dyskryminatora\n",
    "def get_discriminator(\n",
    "    input_shape=(32, 32, 3), classes: int = 10, noise_size: int = 64\n",
    ") -> models.Model:\n",
    "    inputs = layers.Input(shape=input_shape, dtype=tf.float32, name=\"images\")\n",
    "    aux_inputs = layers.Input(shape=[1], dtype=tf.int32, name=\"category\")\n",
    "\n",
    "    y = layers.Embedding(classes, noise_size)(aux_inputs)\n",
    "\n",
    "    x = inputs\n",
    "    features: Sequence[layers.Layer] = []\n",
    "    for filters in [64, 128, 256]:\n",
    "        z = layers.Dense(filters, activation=\"relu\")(y)\n",
    "        z = layers.Reshape([1, 1, -1])(z)\n",
    "\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.Add()([x, z])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        features.append(x)\n",
    "\n",
    "    x = layers.Conv2D(1, 3, padding=\"same\")(x)  # brak aktywacji\n",
    "\n",
    "    # Model ma teraz kilka wyjść\n",
    "    return models.Model(\n",
    "        inputs=[inputs, aux_inputs], outputs=features + [x], name=\"discriminator\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GAN(models.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classes: int,\n",
    "        noise_size: int,\n",
    "        image_size: Tuple[int, int, int],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Tworznie generatora i dyskryminatora w konstruktorze\n",
    "        self.generator = get_generator(noise_size=noise_size, classes=classes)\n",
    "        self.discriminator = get_discriminator(\n",
    "            input_shape=image_size, classes=classes, noise_size=noise_size\n",
    "        )\n",
    "\n",
    "        # Budowanie modeli\n",
    "        self.generator.build(input_shape=[(None, noise_size), (None, 1)])\n",
    "        self.discriminator.build(input_shape=[(None, *image_size), (None, 1)])\n",
    "\n",
    "    def compile(self, *args, **kwargs):\n",
    "        super().compile(*args, **kwargs)\n",
    "        # Kompilacja modeli\n",
    "        self.generator.compile(\n",
    "            optimizer=optimizers.Adam(0.0002, beta_1=0.0, beta_2=0.99),\n",
    "            loss=FeatureMatchingLoss(),\n",
    "        )\n",
    "        self.discriminator.compile(\n",
    "            optimizer=optimizers.Adam(0.0004, beta_1=0.0, beta_2=0.99),\n",
    "            loss=losses.Hinge(),\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.generator(inputs, training=training)\n",
    "\n",
    "    # Kluczowa metoda, która definiuje trenowanie modelu\n",
    "    # Dodanie dekoratora `@tf.function` przyspiesza trenowanie\n",
    "    # @tf.function(jit_compile=True)\n",
    "    def train_step(self, data):\n",
    "        real_images, real_labels = data\n",
    "\n",
    "        bs = tf.shape(real_images)[0]\n",
    "\n",
    "        noise = tf.random.normal((bs, noise_size))\n",
    "        fake_images = self.generator([noise, real_labels])\n",
    "\n",
    "        # Trenowanie dyskryminatora\n",
    "        self.discriminator.trainable = True\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Ostatnie wyjście to wynik dlatego [-1]\n",
    "            real_output = self.discriminator([real_images, real_labels])[-1]\n",
    "            fake_output = self.discriminator([fake_images, real_labels])[-1]\n",
    "\n",
    "            hinge_real = tf.ones_like(real_output)\n",
    "            hinge_fake = -tf.ones_like(fake_output)\n",
    "\n",
    "            disc_loss_real = self.discriminator.loss(hinge_real, real_output)\n",
    "            disc_loss_fake = self.discriminator.loss(hinge_fake, fake_output)\n",
    "            total_d_loss = disc_loss_real + disc_loss_fake\n",
    "\n",
    "        grads = tape.gradient(total_d_loss, self.discriminator.trainable_variables)\n",
    "        self.discriminator.optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Trenowanie generatora\n",
    "        self.discriminator.trainable = False\n",
    "        with tf.GradientTape() as tape:\n",
    "            noise = tf.random.normal((bs, noise_size))\n",
    "            fake_images = self.generator([noise, real_labels])\n",
    "\n",
    "            fake_output = self.discriminator([fake_images, real_labels])\n",
    "            real_output = self.discriminator([real_images, real_labels])\n",
    "\n",
    "            mean_loss = -tf.reduce_mean(fake_output[-1])\n",
    "            fm_loss = self.generator.loss(real_output, fake_output)\n",
    "\n",
    "            total_g_loss = mean_loss + fm_loss\n",
    "\n",
    "        grads = tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.generator.optimizer.apply_gradients(\n",
    "            zip(grads, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        return {\"d_loss\": total_d_loss, \"g_loss\": total_g_loss}\n",
    "\n",
    "\n",
    "class PlotImagesCallback(callbacks.Callback):\n",
    "    def __init__(self, noise_size: int, classes: int):\n",
    "        self.noise = tf.random.normal((10, noise_size))\n",
    "        self.labels = tf.constant([[i] for i in range(10)], dtype=tf.int32)\n",
    "        self.classes = classes\n",
    "        self.class_names = [\n",
    "            \"plane\",\n",
    "            \"car\",\n",
    "            \"bird\",\n",
    "            \"cat\",\n",
    "            \"deer\",\n",
    "            \"dog\",\n",
    "            \"frog\",\n",
    "            \"horse\",\n",
    "            \"ship\",\n",
    "            \"truck\",\n",
    "        ]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        images = self.model.generator([self.noise, self.labels])\n",
    "\n",
    "        plt.figure(figsize=(10, 1))\n",
    "        for i in range(10):\n",
    "            plt.subplot(1, 10, i + 1)\n",
    "            plt.imshow(images[i])\n",
    "            plt.title(self.class_names[i])\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Tworzenie modelu GAN\n",
    "classes = 10\n",
    "noise_size = 64\n",
    "image_size = (32, 32, 3)\n",
    "\n",
    "\n",
    "# Wczytanie cifar10\n",
    "(train_images, train_labels), (_, _) = datasets.cifar10.load_data()\n",
    "train_images = train_images / 255.0\n",
    "train_labels = train_labels.reshape(-1, 1)\n",
    "\n",
    "# Trenowanie modelu\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "# Tworzenie zbioru tf.data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "\n",
    "gan = GAN(classes, noise_size, image_size)\n",
    "gan.compile()\n",
    "\n",
    "gan.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    epochs=epochs,\n",
    "    callbacks=[PlotImagesCallback(noise_size, classes)],\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfjs-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
