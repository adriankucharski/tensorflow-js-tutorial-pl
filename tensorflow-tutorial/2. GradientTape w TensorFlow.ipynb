{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientTape - Serce TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GradientTape` to główny mechanizm TensorFlow, który pozwala na obliczanie gradientów funkcji definiowanych przy pomocy Kerasa lub TensorFlow. Dopiero pełne zrozumienie `GradientTape` pozwala na pełne wykorzystanie potencjału TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers, losses, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "# Generacja cifar10 przy użyciu GANów\n",
    "\n",
    "\n",
    "# Tworzenie generatora\n",
    "def get_generator(noise_size: int = 64, classes: int = 10) -> models.Model:\n",
    "    inputs = layers.Input(shape=[noise_size], dtype=tf.float32, name=\"noise\")\n",
    "    aux_inputs = layers.Input(shape=[1], dtype=tf.int32, name=\"category\")\n",
    "\n",
    "    # Embedding dla kategorii\n",
    "    y = layers.Embedding(classes, noise_size)(aux_inputs)\n",
    "\n",
    "    x = layers.Add()([inputs, y])\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape([4, 4, -1])(x)  # -1 automatycznie dobiera wymiar\n",
    "\n",
    "    for filters in [512, 256, 128]:\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        x = layers.UpSampling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(3, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "    return models.Model(inputs=[inputs, aux_inputs], outputs=x, name=\"generator\")\n",
    "\n",
    "\n",
    "# Tworzenie dyskryminatora\n",
    "def get_discriminator(\n",
    "    input_shape=(32, 32, 3), classes: int = 10, noise_size: int = 64\n",
    ") -> models.Model:\n",
    "    inputs = layers.Input(shape=input_shape, dtype=tf.float32, name=\"images\")\n",
    "    aux_inputs = layers.Input(shape=[1], dtype=tf.int32, name=\"category\")\n",
    "\n",
    "    y = layers.Embedding(classes, noise_size)(aux_inputs)\n",
    "\n",
    "    x = inputs\n",
    "    for filters in [64, 128, 256]:\n",
    "        z = layers.Dense(filters, activation=\"relu\")(y)\n",
    "        z = layers.Reshape([1, 1, -1])(z)\n",
    "\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.Add()([x, z])\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(1, 3, padding=\"same\")(x)  # brak aktywacji\n",
    "\n",
    "    return models.Model(inputs=[inputs, aux_inputs], outputs=x, name=\"discriminator\")\n",
    "\n",
    "\n",
    "# Tworzenie modelu GAN\n",
    "classes = 10\n",
    "noise_size = 64\n",
    "image_size = (32, 32, 3)\n",
    "\n",
    "generator = get_generator(noise_size=noise_size, classes=classes)\n",
    "disciminator = get_discriminator(\n",
    "    input_shape=image_size, classes=classes, noise_size=noise_size\n",
    ")\n",
    "\n",
    "disciminator.build(input_shape=[(None, *image_size), (None, 1)])\n",
    "generator.build(input_shape=[(None, noise_size), (None, 1)])\n",
    "\n",
    "disciminator.compile(\n",
    "    optimizer=optimizers.Adam(0.0001, beta_1=0.0, beta_2=0.99),\n",
    "    loss=losses.Hinge(),\n",
    ")\n",
    "generator.compile(\n",
    "    optimizer=optimizers.Adam(0.0001, beta_1=0.0, beta_2=0.99),\n",
    "    loss=[losses.BinaryCrossentropy(from_logits=True), losses.MeanSquaredError()],\n",
    ")\n",
    "\n",
    "# disciminator.summary()\n",
    "# generator.summary()\n",
    "\n",
    "# Wczytanie cifar10\n",
    "(train_images, train_labels), (_, _) = datasets.cifar10.load_data()\n",
    "train_images = train_images / 255.0\n",
    "train_labels = train_labels.reshape(-1, 1)\n",
    "\n",
    "# Trenowanie modelu\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    discriminator_losses = []\n",
    "    generator_losses = []\n",
    "\n",
    "    with tqdm.tqdm(total=len(train_images)) as pbar:\n",
    "        for i in range(0, len(train_images), batch_size):\n",
    "            # Pobieranie batcha\n",
    "            real_labels = tf.convert_to_tensor(\n",
    "                train_labels[i : i + batch_size], dtype=tf.int32\n",
    "            )\n",
    "            real_images = tf.convert_to_tensor(\n",
    "                train_images[i : i + batch_size], dtype=tf.float32\n",
    "            )\n",
    "            bs = tf.shape(real_images)[0]\n",
    "\n",
    "            # Przygotowanie danych dla generatora\n",
    "            noise = tf.random.normal((bs, noise_size))\n",
    "            # Nie losujemy kategorii, aby ułatwić pracę dyskryminatorowi\n",
    "            fake_images = generator([noise, real_labels])\n",
    "\n",
    "            # Trenowanie dyskryminatora przy pomocy `GradientTape`\n",
    "            # Włączamy trenowanie dyskryminatora\n",
    "            disciminator.trainable = True\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Dyskyminator ocenia obrazy prawdziwe i fałszywe\n",
    "                real_output = disciminator([real_images, real_labels])\n",
    "                fake_output = disciminator([fake_images, real_labels])\n",
    "\n",
    "                # Definiowanie etykiet dla funkcji straty Hinge\n",
    "                hinge_real = tf.ones_like(real_output)  # 1\n",
    "                hinge_fake = -tf.ones_like(fake_output)  # -1\n",
    "\n",
    "                # Obliczanie straty\n",
    "                disc_loss_real = disciminator.loss(hinge_real, real_output)\n",
    "                disc_loss_fake = disciminator.loss(hinge_fake, fake_output)\n",
    "                total_loss = disc_loss_real + disc_loss_fake\n",
    "\n",
    "            discriminator_losses.append(total_loss.numpy())\n",
    "            # Obliczanie gradientów\n",
    "            grads = tape.gradient(total_loss, disciminator.trainable_variables)\n",
    "            # Aktualizacja wag\n",
    "            disciminator.optimizer.apply_gradients(\n",
    "                zip(grads, disciminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "            # Trenowanie generatora\n",
    "            # Wyłączamy trenowanie dyskryminatora\n",
    "            disciminator.trainable = False\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generujemy obrazy ponownie\n",
    "                noise = tf.random.normal((bs, noise_size))\n",
    "                fake_images = generator([noise, real_labels])\n",
    "\n",
    "                # Oceniamy obrazy przez dyskryminator\n",
    "                fake_output = disciminator([fake_images, real_labels])\n",
    "\n",
    "                # Obliczanie straty\n",
    "                # Tworzenie etykiet dla funkcji strat BinaryCrossentropy\n",
    "                bce_real = tf.ones_like(fake_output)\n",
    "\n",
    "                # Obliczanie straty dla generatora przez dyskryminator\n",
    "                bce_gen_loss = generator.loss[0](bce_real, fake_output)\n",
    "                # Obliczanie straty dla generatora bez dyskryminatora\n",
    "                mse_gen_loss = generator.loss[1](real_images, fake_images)\n",
    "                total_loss = bce_gen_loss + mse_gen_loss\n",
    "\n",
    "            generator_losses.append(total_loss.numpy())\n",
    "            # Obliczanie gradientów\n",
    "            grads = tape.gradient(total_loss, generator.trainable_variables)\n",
    "            # Aktualizacja wag\n",
    "            generator.optimizer.apply_gradients(\n",
    "                zip(grads, generator.trainable_variables)\n",
    "            )\n",
    "\n",
    "            pbar.set_description(\n",
    "                f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "                f\"Discriminator loss: {sum(discriminator_losses) / len(discriminator_losses):.4f}, \"\n",
    "                f\"Generator loss: {sum(generator_losses) / len(generator_losses):.4f}\"\n",
    "            )\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "    # Wyświetlanie przykładowych obrazów\n",
    "    noise = tf.random.normal((10, noise_size))\n",
    "    labels = tf.constant([[i] for i in range(10)], dtype=tf.int32)\n",
    "    images = generator([noise, labels])\n",
    "\n",
    "    plt.figure(figsize=(10, 1))\n",
    "    for i in range(10):\n",
    "        plt.subplot(1, 10, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfjs-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
