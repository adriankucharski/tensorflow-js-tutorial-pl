{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje strat i optymalizatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobnie jak `PyTorch`, `Keras` posiada wiele wbudowanych funkcji strat i optymalizatorów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers, losses, models, layers\n",
    "\n",
    "## Optymalizatory\n",
    "optimizers.Adam\n",
    "optimizers.SGD\n",
    "optimizers.RMSprop\n",
    "optimizers.Adagrad\n",
    "optimizers.Adadelta\n",
    "optimizers.Adamax\n",
    "optimizers.Nadam\n",
    "optimizers.AdamW\n",
    "# ...\n",
    "\n",
    "## Funkcje straty (klasyfikacja)\n",
    "losses.BinaryCrossentropy\n",
    "losses.CategoricalCrossentropy\n",
    "losses.CategoricalHinge\n",
    "losses.SquaredHinge\n",
    "losses.Hinge\n",
    "losses.CategoricalFocalCrossentropy\n",
    "losses.SparseCategoricalCrossentropy\n",
    "# ...\n",
    "\n",
    "## Funkcje straty (regresja)\n",
    "losses.MeanSquaredError\n",
    "losses.MeanAbsoluteError\n",
    "losses.Huber\n",
    "losses.LogCosh\n",
    "losses.Tversky\n",
    "losses.Dice\n",
    "# ...\n",
    "\n",
    "# ---------- Przykład ----------\n",
    "\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=optimizers.RMSprop(\n",
    "        learning_rate=0.001,\n",
    "        rho=0.9,\n",
    "        momentum=0.01,\n",
    "        epsilon=1e-07,\n",
    "        centered=False,\n",
    "        weight_decay=0.0,\n",
    "    ),\n",
    "    loss=losses.CategoricalCrossentropy(\n",
    "        from_logits=False,\n",
    "        label_smoothing=0.1,\n",
    "        axis=-1,\n",
    "        reduction=\"sum_over_batch_size\",\n",
    "        name=\"categorical_crossentropy\",\n",
    "    ),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# model.fit(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Własna funkcja strat\n",
    "\n",
    "Najprostsza funkcja strat to zwykła pythonowa funkcja, która przyjmuje dwa argumenty: `y_true` i `y_pred` i zwraca wartość straty. \n",
    "\n",
    "```python\n",
    "from keras import ops\n",
    "def my_loss(y_true, y_pred):\n",
    "    return ops.mean(ops.square(y_true - y_pred), axis=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import ops, optimizers, models, layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def my_squared_error(y_true, y_pred):\n",
    "    return ops.mean(ops.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((4, 4)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(10),  # bez aktywacji\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "    loss=my_squared_error,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(np.random.rand(100, 28, 28, 1), np.random.rand(100, 10), epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Własna funkcja strat z parametrami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import ops, optimizers, models, layers, datasets, losses\n",
    "\n",
    "\n",
    "def my_cce_loss_with_param(\n",
    "    label_smoothing: float = 0.0, from_logits: bool = False, axis: int = -1\n",
    "):\n",
    "    def apply(y_true, y_pred):\n",
    "        target = ops.convert_to_tensor(y_true)\n",
    "        if label_smoothing > 0.0:\n",
    "            num_classes = ops.cast(ops.shape(target)[-1], y_pred.dtype)\n",
    "            target = y_true * (1.0 - label_smoothing) + (label_smoothing / num_classes)\n",
    "\n",
    "        output = ops.convert_to_tensor(y_pred)\n",
    "        if from_logits:\n",
    "            log_prob = ops.log_softmax(output)\n",
    "        else:\n",
    "            output = output / ops.sum(output, axis=axis, keepdims=True)\n",
    "            output = ops.clip(output, 1e-6, 1.0 - 1e-6)\n",
    "            log_prob = ops.log(output)\n",
    "\n",
    "        return -ops.sum(target * log_prob, axis=axis)\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((4, 4)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(10),  # bez aktywacji\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "    loss=my_cce_loss_with_param(label_smoothing=0.1, from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "y_train = ops.one_hot(y_train, 10)\n",
    "\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "y_test = ops.one_hot(y_test, 10)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Własna funkcja straty - obiektowo\n",
    "\n",
    "Jest to preferowany i zalecany przeze mnie sposób tworzenia własnych funkcji straty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03125, 0.0625, 0.125, 0.25, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=148.60935974121094>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import losses, applications, ops, models, datasets\n",
    "from typing import Literal, Sequence\n",
    "\n",
    "\n",
    "class VGGFeatureMatchingLoss(losses.Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vgg_model: Literal[\"VGG16\", \"VGG19\"] = \"VGG19\",\n",
    "        encoder_layers: Sequence[str] = [\n",
    "            \"block1_conv1\",\n",
    "            \"block2_conv1\",\n",
    "            \"block3_conv1\",\n",
    "            \"block4_conv1\",\n",
    "            \"block5_conv1\",\n",
    "        ],\n",
    "        layer_weights: Sequence[float] = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1],\n",
    "        diff_fn=losses.MeanAbsoluteError(),\n",
    "        resize: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(VGGFeatureMatchingLoss, self).__init__(**kwargs)\n",
    "\n",
    "        self.weights = layer_weights\n",
    "        self.resize = resize\n",
    "        self.diff_fn = diff_fn\n",
    "\n",
    "        if vgg_model == \"VGG16\":\n",
    "            vgg = applications.VGG16(include_top=False, weights=\"imagenet\")\n",
    "            self.preprocess = applications.vgg16.preprocess_input\n",
    "        elif vgg_model == \"VGG19\":\n",
    "            vgg = applications.VGG19(include_top=False, weights=\"imagenet\")\n",
    "            self.preprocess = applications.vgg19.preprocess_input\n",
    "        else:\n",
    "            raise ValueError(\"Invalid VGG model\")\n",
    "\n",
    "        layer_outputs = [vgg.get_layer(x).output for x in encoder_layers]\n",
    "        self.vgg_model = models.Model(vgg.input, layer_outputs, name=vgg_model)\n",
    "        self.vgg_model.trainable = False\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true, y_pred = 127.5 * (y_true + 1), 127.5 * (y_pred + 1)\n",
    "\n",
    "        if self.resize:\n",
    "            y_true = ops.image.resize(\n",
    "                y_true, (224, 224), interpolation=\"bilinear\", antialias=True\n",
    "            )\n",
    "            y_pred = ops.image.resize(\n",
    "                y_pred, (224, 224), interpolation=\"bilinear\", antialias=True\n",
    "            )\n",
    "\n",
    "        if y_true.shape[-1] == 1 and y_pred.shape[-1] == 1:\n",
    "            y_true = ops.repeat(y_true, 3, axis=-1)\n",
    "            y_pred = ops.repeat(y_pred, 3, axis=-1)\n",
    "\n",
    "        y_true = self.preprocess(y_true)\n",
    "        y_pred = self.preprocess(y_pred)\n",
    "\n",
    "        real_features = self.vgg_model(y_true)\n",
    "        fake_features = self.vgg_model(y_pred)\n",
    "\n",
    "        loss = 0.0\n",
    "        for i, w in enumerate(self.weights):\n",
    "            real, fake = real_features[i], fake_features[i]\n",
    "            loss += w * self.diff_fn(real, fake)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "(x_train, _), _ = datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train[:16].astype(\"float32\") / 127.5 - 1\n",
    "\n",
    "loss = VGGFeatureMatchingLoss(resize=True)\n",
    "\n",
    "loss(x_train, x_train), loss(x_train[:8], x_train[8:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfjs-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
